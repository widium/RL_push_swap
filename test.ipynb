{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import nan\n",
    "\n",
    "from tabulate import tabulate\n",
    "import random\n",
    "from itertools import zip_longest \n",
    "from collections import namedtuple\n",
    "from stack import Stack\n",
    "from actions import Moove, Action\n",
    "from utils_class import EpsilonGreedy, ReplayMemory, Agent\n",
    "from DQN import DQNAgent\n",
    "from env import Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Experience(state=1, action=1, next_state=1, reward=1),\n",
       " Experience(state=2, action=2, next_state=2, reward=2),\n",
       " Experience(state=3, action=3, next_state=3, reward=3)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experience = namedtuple('Experience', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "e1 = Experience(1,1,1,1)\n",
    "e2 = Experience(2,2,2,2)\n",
    "e3 = Experience(3,3,3,3)\n",
    "\n",
    "experiences = [e1,e2,e3]\n",
    "experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=(1, 2, 3), action=(1, 2, 3), next_state=(1, 2, 3), reward=(1, 2, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = Experience(*zip(*experiences))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(experiences):\n",
    "    \n",
    "    batch = Experience(*zip(*experiences))\n",
    "    \n",
    "    state = np.array([state for state in batch.state])\n",
    "    action = np.array([action for action in batch.action])\n",
    "    next_state = np.array([next_state for next_state in batch.next_state])\n",
    "    reward = np.array([reward for reward in batch.reward])\n",
    "    \n",
    "    return state, action, next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action, next_state, reward = extract_value(experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1 1 1\n",
      "1 2 2 2 2\n",
      "2 3 3 3 3\n"
     ]
    }
   ],
   "source": [
    "for t, (state, action, next_state, reward) in enumerate(experiences):\n",
    "    print(t, state, action, next_state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7af6488317c4eae45cfe2d92ddcd760ac10ac76eee454fa0eead8075769044a8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
