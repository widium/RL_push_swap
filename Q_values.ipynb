{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STACK_SIZE = 10\n",
    "\n",
    "EPISODE = 2\n",
    "CAPACITY = 100_000\n",
    "BATCH_SIZE = 3\n",
    "MIN_REPLAY_MEMORY_SIZE = 100\n",
    "\n",
    "DECAY = 0.001\n",
    "START_EPSILON = 1\n",
    "MIN_EPSILON = 0.15\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "ACTION_REWARD = -1\n",
    "INVERSE_ACTION_REWARD = -5\n",
    "FINISH_REWARD = 100\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Values:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_model, states, actions):\n",
    "        return policy_model.predict(states)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_model, next_states):\n",
    "        return target_model.predict(next_states)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, states, next_states, experiences, policy_model, target_model):\n",
    "    \n",
    "    ##--------Recuperer toutes les Q values des 2 model-----##\n",
    "    Q_policy_list = policy_model.predict(states)\n",
    "    Q_target_list = target_model.predict(next_states)\n",
    "    \n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    ##--------Creer X et Y pour l'entrainement du model-----##\n",
    "    for t, (states, action, next_state, reward) in enumerate(experiences):\n",
    "      \n",
    "      #tant que c'est pas la dernier q value\n",
    "      #calculer la q value actuel avec la belman equation\n",
    "      if t < BATCH_SIZE:\n",
    "        \n",
    "        #recuperer la max Q_target & calculer sa Qvalue a linstant t\n",
    "        max_Q_target = np.max(Q_target_list[t])\n",
    "        max_Q_value = reward + GAMMA * max_Q_target\n",
    "      \n",
    "      #si c'est la derniere q value on lui assigne un reward\n",
    "      else :\n",
    "        max_Q_value = reward\n",
    "\n",
    "      #recuperer les Q value a l'instant t [11]\n",
    "      Q_target = Q_policy_list[t]\n",
    "      #assigner la meilleur actions  [1]\n",
    "      Q_target[action] = max_Q_value\n",
    "\n",
    "      #Ajouter les State dans X\n",
    "      #Ajouter les Q_value avec l'argmax.\n",
    "      X.append(states)\n",
    "      y.append(Q_target)\n",
    "\n",
    "    policy_model.fit(np.array(X), np.array(y), batch_size=BATCH_SIZE, verbose=1, shuffle=False) \n",
    "\n",
    "    # target_update_counter += 1\n",
    "\n",
    "    # If counter reaches set value, update target network with weights of main network\n",
    "    target_model.set_weights(policy_model.get_weights())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7af6488317c4eae45cfe2d92ddcd760ac10ac76eee454fa0eead8075769044a8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
